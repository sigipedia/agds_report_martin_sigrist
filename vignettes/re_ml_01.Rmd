---
title: "Supervised Machine Learning I"
author: "Maritn Sigrist (21-101-175)"
date: "2023-05-25"
output: html_document
---

# Load required Libraries
```{r, message=FALSE}
library(tidyverse)
library(lubridate)
library(caret)
library(recipes)
```

# Load external functions
```{r}
source("../R/eval_model.R")
source("../R/read_data.R")
```


## Read and Clean Data
```{r}
daily_fluxes <- read_data("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")
```

## Data spliting 
```{r}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
```

## Model and pre-processing formulation
```{r}
# Use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F + VPD_F + PA_F + P_F + WS_F,
                      data = daily_fluxes_train |> drop_na()) |>
  recipes::step_BoxCox(all_predictors()) |>
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```


# Comparison of the linear regression and KNN models
## Linear regression model
```{r}
# Fit linear regression model
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_lm <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "lm",
    trControl = caret::trainControl(method = "none"),
    metric = "RMSE"
  )
)

eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

The graph shows the predicted vs. measured GPP for the linear OLS model. In the left scatter plot shows the training data and the right one right one the results from the test data. The blue line in the plot has slope one and no offset. All points of the training and the test data would lie on this line with a perfect model. The red dashed line is the linear regression of the corresponding points. The coefficient of determination (R2) and the root mean square error (RMSE) are also shown in the graph. There is an important difference between the two metrics. R2 is calculated as in any other linear regression. Even with a perfect linear correlation - R2 equal one and all points on the red line - depending on the model, there could still be an offset in predicted values. The RMSE value is calculated against the blue line.
The comparison of the results from the training and the test data show similar values for both metrics. Identical RMSE values is an indication for a good bias-variance trade-off. The slope of the regression line is smaller than the one from an ideal model. This indicates that the model shows the tendency to underestimate higher GPP values and to overestimate lower values.

## k-Neares-Neighbour model
```{r}
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = 8),
    metric = "RMSE"
  )
)

eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

The first k-Nearest-Neighbour (KNN) model shows similar results as the linear model. All metrics values are within the same range. The RMSE values are both smaller and the R2 values higher than on the linear model. The prediction accuracy of the KNN model is better. The difference of the RMSE values between the training and the test data is an indication that the bias-variance trade-off is not perfect. The model show better result on the training data than on the test data. The model is overfitted and it's ability for generalization is not ideal.
Also this model show the tendency to underestimate higher GPP values and to overestimate lower values.


##  Temporal variations of observed and modelled GPP
```{r}
# Prepare data frame for plotting
plot_data <- daily_fluxes |>
  select(-LW_IN_F) |> # Remove LW_IN_F variable
  drop_na() |> # Remove observations with NAs
  select(TIMESTAMP, GPP_NT_VUT_REF) |> # Select time stamp and reference variables
  mutate(lm_pred = predict(mod_lm, newdata = daily_fluxes |> select(-LW_IN_F) |> drop_na())) |> # Add prediction of linear model
  mutate(knn_pred = predict(mod_knn, newdata = daily_fluxes |> select(-LW_IN_F) |> drop_na())) # Add prediction of knn model
```

### Full time series
#### Linear Model
```{r}
# Create plot of measured data
gpp_meas <- ggplot(
  data = plot_data,
  aes(x = TIMESTAMP, y = GPP_NT_VUT_REF)) +
  geom_line() +
  ylim(0, 10) +
  labs(title = "Measured data",
       x = NULL,
       y = expression(atop("GPP_NT_VUT_REF", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot of predicted GPP by linear model
gpp_lm <- ggplot(
  data = plot_data,
  aes(x = TIMESTAMP, y = lm_pred)) +
  geom_line() +
  ylim(0, 10) +
  labs(title = "Linear model prediction",
       x = NULL,
       y = expression(atop("GPP Prediction", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot with residua for linear model
lm_res <- ggplot(
    data = plot_data,
    aes(x = TIMESTAMP, y = lm_pred-GPP_NT_VUT_REF)) +
  geom_line() +
  geom_hline(yintercept=0, linetype='dashed', color="red", linewidth=1.5) +
  labs(title = "Linear model residua", 
       x = "Date", 
       y = expression(atop("Residua", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# combine plots
suppressWarnings( # Suppress warnings
  cowplot::plot_grid(gpp_meas, gpp_lm, lm_res, nrow = 3)
)
```

The measures data show a clear annual variation. The prediction of the linear model is able to predict an annual cycle. Comparing the temporal GPP series between measured and predicted values by the residua confirm that higher values are underestimated.


#### k-Nearest-Neighbor Model
```{r}
# Create plot of predicted GPP by KNN model
gpp_knn <- ggplot(
  data = plot_data,
  aes(x = TIMESTAMP, y = knn_pred)) +
  geom_line() +
  ylim(0, 10) +
  labs(title = "KNN model prediction",
       x = NULL,
       y = expression(atop("GPP Prediction", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot with residua for KNN model
knn_res <- ggplot(
    data = plot_data,
    aes(x = TIMESTAMP, y = knn_pred-GPP_NT_VUT_REF)) +
  geom_line() +
  geom_hline(yintercept=0, linetype='dashed', color="red", linewidth=1.5) +
  labs(title = "KNN model residua", 
       x = "Date", 
       y = expression(atop("Residua", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# combine plots
suppressWarnings( # Suppress warnings
  cowplot::plot_grid(gpp_meas, gpp_knn, knn_res, nrow = 3)
)
```

The temporal variation predicted by the KNN model shows a similar pattern as the linear model.


### Day of year mean values

Comparing the mean value for each day of the year will give closer look on the annual variation.

```{r}
# Prepare data frame for plotting
plot_data_doy <- plot_data |>
  mutate(doy = yday(TIMESTAMP)) |>            # Add new column with day of the year
  group_by(doy) |>                            # Group data by day of the year
  summarise(GPP_doy = mean(GPP_NT_VUT_REF),   # Mean of measured GPP value over all years
            knn_pred_doy = mean(knn_pred),    # Mean of KNN model prediction over all years
            lm_pred_doy = mean(lm_pred))      # Mean of KNN model prediction over all years
```

#### Linear Model
```{r}
# Create plot of measured data
gpp_meas_doy <- ggplot(
  data = plot_data_doy,
  aes(x = doy, y = GPP_doy)) +
  geom_line() +
  labs(title = "Measured data",
       x = NULL,
       y = expression(atop("GPP (mean)", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot of predicted GPP by linear model
gpp_lm_doy <- ggplot(
  data = plot_data_doy,
  aes(x = doy, y = lm_pred_doy)) +
  geom_line() +
  labs(title = "Linear model prediction",
       x = NULL,
       y = expression(atop("GPP (mean)", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot with residua for linear model
lm_res_doy <- ggplot(
    data = plot_data_doy,
    aes(x = doy, y = lm_pred_doy-GPP_doy)) +
  geom_line() +
  geom_hline(yintercept=0, linetype='dashed', color="red", linewidth=1.5) +
  labs(title = "Linear model residua", 
       x = "Day of year", 
       y = expression(atop("GPP (mean)", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# combine plots
cowplot::plot_grid(gpp_meas_doy, gpp_lm_doy, lm_res_doy, nrow = 3)
```

The aggregation for each day of the year clearly show that the model is underestimating higher and overestimating lower values. In average, predicted values for spring-time are too high and for summer-time too low.

#### KNN Model
```{r}
# Create plot of predicted GPP by linear model
gpp_knn_doy <- ggplot(
  data = plot_data_doy,
  aes(x = doy, y = knn_pred_doy)) +
  geom_line() +
  labs(title = "KNN model prediction",
       x = NULL,
       y = expression(atop("GPP (mean)", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# Create plot with residua for KNN model
knn_res_doy <- ggplot(
    data = plot_data_doy,
    aes(x = doy, y = knn_pred_doy-GPP_doy)) +
  geom_line() +
  geom_hline(yintercept=0, linetype='dashed', color="red", linewidth=1.5) +
  labs(title = "KNN model residua", 
       x = "Day of year", 
       y = expression(atop("GPP (mean)", paste("(", mu,"mol CO"[2], " m"^-2, "s"^-1, ")")))) +
  theme_classic() +
  theme(axis.title.y = element_text(size = 10))

# combine plots
cowplot::plot_grid(gpp_meas_doy, gpp_knn_doy, knn_res_doy, nrow = 3)
```

The KNN model have the same tendency as the linear model.


# The role of k
## Hypothesis

Setting the value for k to one would results in a perfect correlation on the training data. Predicting GPP from an observation of the training set would result in the measured value, because only this single point would be considered. R2 would become 1 and RMSE would become 0. An overfitted model has low bias and a high variance. The metrics of the test data would give a small R2 and a high RMSE.

Taking a hig value for k would reslut in an underfitted model. For the case that k is equal the number of observations (N), every prediction would give the same result, the average of the training data. For both sets - training and validation - R2 would become 0 and RMSE would be big.


## Model with k = 1
```{r}
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = 1),
    metric = "RMSE"
  )
)

eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

The results of the KNN model with k equal one confirm the first hypothesis.

## Model with high k
```{r}
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = 400),
    metric = "RMSE"
  )
)

eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

The KNN model with a high value chosen for k, shows the hypothesized result. Compared to the KNN model in the previous chapter, R2 is lower and RMSE higher for both data sets.


## Evaluate dependency from k

In the last step dependency from k on the KNN model results is investigated. Two different methods are used for tuning the hyperparameter k.

### Method: Cross validation
```{r}
set.seed(42) # Set random number generator seed for reproducibility

# Tune hyperparameter k with cross validation
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp,
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "cv",
                                    number = 10,
                                    search = "grid"),
    tuneGrid = expand.grid(k = c(1, 2, 5, 10, 15, 18, 19, 20, 21, 22, 23, 24, 25, 50, 100, 200)),
    metric = "RMSE"
  )
)

# Create plot
ggplot(
    data = mod_knn$results,
    aes(x = k, y = RMSE)) +
  geom_line(linewidth = 1) +
  geom_point(
    data = mod_knn$results |> filter(k == mod_knn$bestTune[1,1]) |> select(k, RMSE),
    aes(x = k, y = RMSE), size = 3, color = "tomato") +
  geom_text(x=25, y=1.6, label = paste("k = ", mod_knn$bestTune[1,1])) +
  labs(title = "RMSE vs. k by Cross Validation", 
       x = "k", 
       y = "RMSE") +
  theme_classic()

```

Conducting a grid search with a 10-fold cross validation and RMSE as metric, k has a value of 23 with the given random number generator seed.


### Method: Bootstrap
```{r}
set.seed(42) # Set random number generator seed for reproducibility

# Tune hyperparameter k with bootstrap
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "boot",
                                    search = "grid"),
    tuneGrid = expand.grid(k = c(1, 2, 5, 10, 15, 20, 30, 31, 32, 33, 34, 35, 36, 37, 40, 45, 50, 100, 200)),
    metric = "RMSE"
  )
)

# Create plot
ggplot(
    data = mod_knn$results,
    aes(x = k, y = RMSE)) +
  geom_line(linewidth = 1) +
  geom_point(
    data = mod_knn$results |> filter(k == mod_knn$bestTune[1,1]) |> select(k, RMSE),
    aes(x = k, y = RMSE), size = 3, color = "tomato") +
  geom_text(x=25, y=1.6, label = paste("k = ", mod_knn$bestTune[1,1])) +
  labs(title = "RMSE vs. k by Bootsrap",
       x = "k", 
       y = "RMSE") +
  theme_classic()
```

The bootstrap method combined with the same metric gives an ideal k value of 31 with the set random number generator seed.


## Model with tuned k

The value for k depends on the method used. For this final evaluation, k is set to the average value.

```{r}
suppressWarnings( # Suppress warnings from Box-Cox-Transformation
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = 27),
    metric = "RMSE"
  )
)

eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

Compared to all previous models, the final model achieves the highest R2 value and the lowest RMSE for the training and the test set. The tuning of the hyperparameter k was successful. Additionally, both values are similar for both sets. This indicates, that the model has a good bias-variance balance.
